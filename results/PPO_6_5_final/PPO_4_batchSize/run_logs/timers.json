{
    "name": "root",
    "gauges": {
        "PPO_6_5_2.Policy.Entropy.mean": {
            "value": 2.935317039489746,
            "min": 2.85073184967041,
            "max": 4.861015796661377,
            "count": 30
        },
        "PPO_6_5_2.Policy.Entropy.sum": {
            "value": 146856.84375,
            "min": 142881.53125,
            "max": 243682.71875,
            "count": 30
        },
        "PPO_6_5_2.Step.mean": {
            "value": 1499954.0,
            "min": 49938.0,
            "max": 1499954.0,
            "count": 30
        },
        "PPO_6_5_2.Step.sum": {
            "value": 1499954.0,
            "min": 49938.0,
            "max": 1499954.0,
            "count": 30
        },
        "PPO_6_5_2.Policy.ExtrinsicValueEstimate.mean": {
            "value": 22.58708381652832,
            "min": 1.0202789306640625,
            "max": 22.694076538085938,
            "count": 30
        },
        "PPO_6_5_2.Policy.ExtrinsicValueEstimate.sum": {
            "value": 18227.77734375,
            "min": 814.1825561523438,
            "max": 18408.41796875,
            "count": 30
        },
        "PPO_6_5_2.Environment.EpisodeLength.mean": {
            "value": 662.7972972972973,
            "min": 375.0229007633588,
            "max": 749.0,
            "count": 30
        },
        "PPO_6_5_2.Environment.EpisodeLength.sum": {
            "value": 49047.0,
            "min": 47187.0,
            "max": 52835.0,
            "count": 30
        },
        "PPO_6_5_2.Environment.CumulativeReward.mean": {
            "value": 759.3984362370259,
            "min": 79.22348832823927,
            "max": 765.9082168779875,
            "count": 30
        },
        "PPO_6_5_2.Environment.CumulativeReward.sum": {
            "value": 56195.48428153992,
            "min": 4991.079764679074,
            "max": 59229.6942615509,
            "count": 30
        },
        "PPO_6_5_2.Policy.ExtrinsicReward.mean": {
            "value": 759.3984362370259,
            "min": 79.22348832823927,
            "max": 765.9082168779875,
            "count": 30
        },
        "PPO_6_5_2.Policy.ExtrinsicReward.sum": {
            "value": 56195.48428153992,
            "min": 4991.079764679074,
            "max": 59229.6942615509,
            "count": 30
        },
        "PPO_6_5_2.Losses.PolicyLoss.mean": {
            "value": 0.03472201300076753,
            "min": 0.030990421030922637,
            "max": 0.03728454460789166,
            "count": 30
        },
        "PPO_6_5_2.Losses.PolicyLoss.sum": {
            "value": 0.17361006500383763,
            "min": 0.12896842955178728,
            "max": 0.1864227230394583,
            "count": 30
        },
        "PPO_6_5_2.Losses.ValueLoss.mean": {
            "value": 2.6906953754027683,
            "min": 1.6467582337676532,
            "max": 108.49143326918283,
            "count": 30
        },
        "PPO_6_5_2.Losses.ValueLoss.sum": {
            "value": 13.453476877013841,
            "min": 6.587032935070613,
            "max": 542.4571663459142,
            "count": 30
        },
        "PPO_6_5_2.Policy.LearningRate.mean": {
            "value": 5.1597382801200016e-06,
            "min": 5.1597382801200016e-06,
            "max": 0.00029464140178619996,
            "count": 30
        },
        "PPO_6_5_2.Policy.LearningRate.sum": {
            "value": 2.5798691400600007e-05,
            "min": 2.5798691400600007e-05,
            "max": 0.0014251092249636004,
            "count": 30
        },
        "PPO_6_5_2.Policy.Epsilon.mean": {
            "value": 0.10171988000000001,
            "min": 0.10171988000000001,
            "max": 0.19821380000000002,
            "count": 30
        },
        "PPO_6_5_2.Policy.Epsilon.sum": {
            "value": 0.5085994,
            "min": 0.4465749333333334,
            "max": 0.9750364,
            "count": 30
        },
        "PPO_6_5_2.Policy.Beta.mean": {
            "value": 6.142441200000001e-05,
            "min": 6.142441200000001e-05,
            "max": 0.0029465926200000006,
            "count": 30
        },
        "PPO_6_5_2.Policy.Beta.sum": {
            "value": 0.00030712206000000004,
            "min": 0.00030712206000000004,
            "max": 0.014253588360000007,
            "count": 30
        },
        "PPO_6_5_2.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        },
        "PPO_6_5_2.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        },
        "PPO_6_5_3.Policy.Entropy.mean": {
            "value": 2.684025287628174,
            "min": 2.5552585124969482,
            "max": 4.900276184082031,
            "count": 30
        },
        "PPO_6_5_3.Policy.Entropy.sum": {
            "value": 134187.84375,
            "min": 127883.1640625,
            "max": 245650.84375,
            "count": 30
        },
        "PPO_6_5_3.Step.mean": {
            "value": 1499993.0,
            "min": 49938.0,
            "max": 1499993.0,
            "count": 30
        },
        "PPO_6_5_3.Step.sum": {
            "value": 1499993.0,
            "min": 49938.0,
            "max": 1499993.0,
            "count": 30
        },
        "PPO_6_5_3.Policy.ExtrinsicValueEstimate.mean": {
            "value": 23.00598907470703,
            "min": 0.7688373923301697,
            "max": 23.00598907470703,
            "count": 30
        },
        "PPO_6_5_3.Policy.ExtrinsicValueEstimate.sum": {
            "value": 18703.869140625,
            "min": 613.5322265625,
            "max": 18729.5625,
            "count": 30
        },
        "PPO_6_5_3.Environment.EpisodeLength.mean": {
            "value": 601.3373493975904,
            "min": 393.640625,
            "max": 749.0,
            "count": 30
        },
        "PPO_6_5_3.Environment.EpisodeLength.sum": {
            "value": 49911.0,
            "min": 47187.0,
            "max": 52565.0,
            "count": 30
        },
        "PPO_6_5_3.Environment.CumulativeReward.mean": {
            "value": 702.5939150775772,
            "min": 61.38380624377538,
            "max": 793.5206230436053,
            "count": 30
        },
        "PPO_6_5_3.Environment.CumulativeReward.sum": {
            "value": 58315.294951438904,
            "min": 3867.179793357849,
            "max": 59438.34463310242,
            "count": 30
        },
        "PPO_6_5_3.Policy.ExtrinsicReward.mean": {
            "value": 702.5939150775772,
            "min": 61.38380624377538,
            "max": 793.5206230436053,
            "count": 30
        },
        "PPO_6_5_3.Policy.ExtrinsicReward.sum": {
            "value": 58315.294951438904,
            "min": 3867.179793357849,
            "max": 59438.34463310242,
            "count": 30
        },
        "PPO_6_5_3.Losses.PolicyLoss.mean": {
            "value": 0.025687224681023507,
            "min": 0.02165754066353353,
            "max": 0.026699496632936644,
            "count": 30
        },
        "PPO_6_5_3.Losses.PolicyLoss.sum": {
            "value": 0.12843612340511754,
            "min": 0.0879866607428994,
            "max": 0.1334974831646832,
            "count": 30
        },
        "PPO_6_5_3.Losses.ValueLoss.mean": {
            "value": 3.8779211926460264,
            "min": 2.0474222709735237,
            "max": 140.7163737742106,
            "count": 30
        },
        "PPO_6_5_3.Losses.ValueLoss.sum": {
            "value": 19.38960596323013,
            "min": 8.189689083894095,
            "max": 703.5818688710531,
            "count": 30
        },
        "PPO_6_5_3.Policy.LearningRate.mean": {
            "value": 5.209818263426659e-06,
            "min": 5.209818263426659e-06,
            "max": 0.0002946414017862,
            "count": 30
        },
        "PPO_6_5_3.Policy.LearningRate.sum": {
            "value": 2.6049091317133296e-05,
            "min": 2.6049091317133296e-05,
            "max": 0.0014251092249635998,
            "count": 30
        },
        "PPO_6_5_3.Policy.Epsilon.mean": {
            "value": 0.10173657333333334,
            "min": 0.10173657333333334,
            "max": 0.19821380000000002,
            "count": 30
        },
        "PPO_6_5_3.Policy.Epsilon.sum": {
            "value": 0.5086828666666667,
            "min": 0.4466936666666668,
            "max": 0.9750364000000001,
            "count": 30
        },
        "PPO_6_5_3.Policy.Beta.mean": {
            "value": 6.192354266666659e-05,
            "min": 6.192354266666659e-05,
            "max": 0.00294659262,
            "count": 30
        },
        "PPO_6_5_3.Policy.Beta.sum": {
            "value": 0.00030961771333333293,
            "min": 0.00030961771333333293,
            "max": 0.01425358836,
            "count": 30
        },
        "PPO_6_5_3.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        },
        "PPO_6_5_3.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        },
        "PPO_6_5.Policy.Entropy.mean": {
            "value": 2.7067136764526367,
            "min": 2.583890438079834,
            "max": 4.834646224975586,
            "count": 30
        },
        "PPO_6_5.Policy.Entropy.sum": {
            "value": 135760.640625,
            "min": 129460.6640625,
            "max": 242360.8125,
            "count": 30
        },
        "PPO_6_5.Step.mean": {
            "value": 1499990.0,
            "min": 49938.0,
            "max": 1499990.0,
            "count": 30
        },
        "PPO_6_5.Step.sum": {
            "value": 1499990.0,
            "min": 49938.0,
            "max": 1499990.0,
            "count": 30
        },
        "PPO_6_5.Policy.ExtrinsicValueEstimate.mean": {
            "value": 22.93137550354004,
            "min": 1.1020076274871826,
            "max": 23.220060348510742,
            "count": 30
        },
        "PPO_6_5.Policy.ExtrinsicValueEstimate.sum": {
            "value": 18941.31640625,
            "min": 879.402099609375,
            "max": 19319.08984375,
            "count": 30
        },
        "PPO_6_5.Environment.EpisodeLength.mean": {
            "value": 465.5943396226415,
            "min": 405.78225806451616,
            "max": 749.0,
            "count": 30
        },
        "PPO_6_5.Environment.EpisodeLength.sum": {
            "value": 49353.0,
            "min": 47187.0,
            "max": 54484.0,
            "count": 30
        },
        "PPO_6_5.Environment.CumulativeReward.mean": {
            "value": 553.0375024057785,
            "min": 81.75602898013497,
            "max": 771.0129688580831,
            "count": 30
        },
        "PPO_6_5.Environment.CumulativeReward.sum": {
            "value": 58621.97525501251,
            "min": 5150.629825748503,
            "max": 61278.52578163147,
            "count": 30
        },
        "PPO_6_5.Policy.ExtrinsicReward.mean": {
            "value": 553.0375024057785,
            "min": 81.75602898013497,
            "max": 771.0129688580831,
            "count": 30
        },
        "PPO_6_5.Policy.ExtrinsicReward.sum": {
            "value": 58621.97525501251,
            "min": 5150.629825748503,
            "max": 61278.52578163147,
            "count": 30
        },
        "PPO_6_5.Losses.PolicyLoss.mean": {
            "value": 0.04747247227314801,
            "min": 0.046369273208529196,
            "max": 0.0526436566830671,
            "count": 30
        },
        "PPO_6_5.Losses.PolicyLoss.sum": {
            "value": 0.23736236136574007,
            "min": 0.1940146667994971,
            "max": 0.2632182834153355,
            "count": 30
        },
        "PPO_6_5.Losses.ValueLoss.mean": {
            "value": 4.0251757448911665,
            "min": 1.749337952571048,
            "max": 82.10599274635317,
            "count": 30
        },
        "PPO_6_5.Losses.ValueLoss.sum": {
            "value": 20.125878724455834,
            "min": 6.997351810284192,
            "max": 410.5299637317658,
            "count": 30
        },
        "PPO_6_5.Policy.LearningRate.mean": {
            "value": 5.167738277453333e-06,
            "min": 5.167738277453333e-06,
            "max": 0.0002946414017862,
            "count": 30
        },
        "PPO_6_5.Policy.LearningRate.sum": {
            "value": 2.5838691387266668e-05,
            "min": 2.5838691387266668e-05,
            "max": 0.0014251092249635998,
            "count": 30
        },
        "PPO_6_5.Policy.Epsilon.mean": {
            "value": 0.10172254666666666,
            "min": 0.10172254666666666,
            "max": 0.19821380000000005,
            "count": 30
        },
        "PPO_6_5.Policy.Epsilon.sum": {
            "value": 0.5086127333333333,
            "min": 0.4466140666666666,
            "max": 0.9750363999999999,
            "count": 30
        },
        "PPO_6_5.Policy.Beta.mean": {
            "value": 6.150414533333334e-05,
            "min": 6.150414533333334e-05,
            "max": 0.0029465926199999997,
            "count": 30
        },
        "PPO_6_5.Policy.Beta.sum": {
            "value": 0.0003075207266666667,
            "min": 0.0003075207266666667,
            "max": 0.01425358836,
            "count": 30
        },
        "PPO_6_5.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        },
        "PPO_6_5.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 30
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1643398047",
        "python_version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\alexc\\anaconda3\\envs\\mlagent_release_18\\Scripts\\mlagents-learn config/PPO/PPO_04_batchSize.yaml --run-id=PPO_04_batchSize --env=builds/PPO_CarAgent6_multi",
        "mlagents_version": "0.27.0",
        "mlagents_envs_version": "0.27.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.19.5",
        "end_time_seconds": "1643402951"
    },
    "total": 4903.201281000001,
    "count": 1,
    "self": 1.0563255000006393,
    "children": {
        "run_training.setup": {
            "total": 0.1811748,
            "count": 1,
            "self": 0.1811748
        },
        "TrainerController.start_learning": {
            "total": 4901.9637807,
            "count": 1,
            "self": 3.722502199970222,
            "children": {
                "TrainerController._reset_env": {
                    "total": 11.9059278,
                    "count": 1,
                    "self": 11.9059278
                },
                "TrainerController.advance": {
                    "total": 4886.053935100029,
                    "count": 173907,
                    "self": 4.55082940018201,
                    "children": {
                        "env_step": {
                            "total": 3264.2424174999855,
                            "count": 173907,
                            "self": 1432.942904399848,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 1829.0782368000423,
                                    "count": 173907,
                                    "self": 24.386641899954384,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 1804.691594900088,
                                            "count": 500112,
                                            "self": 491.8459007000745,
                                            "children": {
                                                "TorchPolicy.sample_actions": {
                                                    "total": 1312.8456942000134,
                                                    "count": 500112,
                                                    "self": 1312.8456942000134
                                                }
                                            }
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.2212763000952283,
                                    "count": 173907,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 4885.343864499971,
                                            "count": 173907,
                                            "is_parallel": true,
                                            "self": 3811.868903699909,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0013841,
                                                    "count": 3,
                                                    "is_parallel": true,
                                                    "self": 0.00048030000000000013,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0009038,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.0009038
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1073.4735767000616,
                                                    "count": 173907,
                                                    "is_parallel": true,
                                                    "self": 47.54540700010875,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 61.59371909993884,
                                                            "count": 173907,
                                                            "is_parallel": true,
                                                            "self": 61.59371909993884
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 779.2985765999526,
                                                            "count": 173907,
                                                            "is_parallel": true,
                                                            "self": 779.2985765999526
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 185.03587400006137,
                                                            "count": 521721,
                                                            "is_parallel": true,
                                                            "self": 65.35285740007558,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 119.68301659998579,
                                                                    "count": 2086884,
                                                                    "is_parallel": true,
                                                                    "self": 119.68301659998579
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 1617.2606881998618,
                            "count": 521721,
                            "self": 13.171987099562102,
                            "children": {
                                "process_trajectory": {
                                    "total": 357.05666860030266,
                                    "count": 521721,
                                    "self": 355.71905350030266,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 1.3376150999999936,
                                            "count": 15,
                                            "self": 1.3376150999999936
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 1247.032032499997,
                                    "count": 435,
                                    "self": 827.1069964000545,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 419.92503609994253,
                                            "count": 30513,
                                            "self": 419.92503609994253
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 8.000006346264854e-07,
                    "count": 1,
                    "self": 8.000006346264854e-07
                },
                "TrainerController._save_models": {
                    "total": 0.2814147999997658,
                    "count": 1,
                    "self": 0.04028640000069572,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.2411283999990701,
                            "count": 3,
                            "self": 0.2411283999990701
                        }
                    }
                }
            }
        }
    }
}