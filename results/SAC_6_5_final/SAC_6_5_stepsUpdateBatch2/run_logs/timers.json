{
    "name": "root",
    "gauges": {
        "SAC_6_5.Policy.Entropy.mean": {
            "value": 2.322266101837158,
            "min": 2.322266101837158,
            "max": 3.413980484008789,
            "count": 26
        },
        "SAC_6_5.Policy.Entropy.sum": {
            "value": 23303.939453125,
            "min": 1372.160888671875,
            "max": 34290.01953125,
            "count": 26
        },
        "SAC_6_5.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5_3.Policy.Entropy.mean": {
            "value": 0.9152762293815613,
            "min": 0.9152762293815613,
            "max": 3.1887965202331543,
            "count": 26
        },
        "SAC_6_5_3.Policy.Entropy.sum": {
            "value": 9044.759765625,
            "min": 628.3055419921875,
            "max": 31942.17578125,
            "count": 26
        },
        "SAC_6_5_3.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5_3.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5_2.Policy.Entropy.mean": {
            "value": 1.9766498804092407,
            "min": 1.8267112970352173,
            "max": 3.347346544265747,
            "count": 26
        },
        "SAC_6_5_2.Policy.Entropy.sum": {
            "value": 19835.681640625,
            "min": 1125.1077880859375,
            "max": 33273.70703125,
            "count": 26
        },
        "SAC_6_5_2.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5_2.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 26
        },
        "SAC_6_5.Step.mean": {
            "value": 499953.0,
            "min": 259949.0,
            "max": 499953.0,
            "count": 25
        },
        "SAC_6_5.Step.sum": {
            "value": 499953.0,
            "min": 259949.0,
            "max": 499953.0,
            "count": 25
        },
        "SAC_6_5.Policy.ExtrinsicValue.mean": {
            "value": 24.050662994384766,
            "min": 19.936124801635742,
            "max": 24.52631950378418,
            "count": 25
        },
        "SAC_6_5.Policy.ExtrinsicValue.sum": {
            "value": 3872.15673828125,
            "min": 3169.84375,
            "max": 3973.794189453125,
            "count": 25
        },
        "SAC_6_5.Losses.PolicyLoss.mean": {
            "value": -72.67997662034858,
            "min": -72.67997662034858,
            "max": -61.27280945354577,
            "count": 25
        },
        "SAC_6_5.Losses.PolicyLoss.sum": {
            "value": -18097.314178466797,
            "min": -18145.10912322998,
            "max": -13459.421155054237,
            "count": 25
        },
        "SAC_6_5.Losses.ValueLoss.mean": {
            "value": 0.024314713638171136,
            "min": 0.013000750272162256,
            "max": 0.04006880488351265,
            "count": 25
        },
        "SAC_6_5.Losses.ValueLoss.sum": {
            "value": 6.0543636959046125,
            "min": 3.2501875680405643,
            "max": 10.177476440412214,
            "count": 25
        },
        "SAC_6_5.Losses.Q1Loss.mean": {
            "value": 0.4052874843316923,
            "min": 0.21953584625309616,
            "max": 0.46334339736402036,
            "count": 25
        },
        "SAC_6_5.Losses.Q1Loss.sum": {
            "value": 100.91658359859139,
            "min": 48.07835032942806,
            "max": 115.83584934100509,
            "count": 25
        },
        "SAC_6_5.Losses.Q2Loss.mean": {
            "value": 0.4026863609354599,
            "min": 0.21569119128672753,
            "max": 0.4578508576030533,
            "count": 25
        },
        "SAC_6_5.Losses.Q2Loss.sum": {
            "value": 100.26890387292951,
            "min": 47.23637089179333,
            "max": 114.46271440076332,
            "count": 25
        },
        "SAC_6_5.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.22875895016044978,
            "min": 0.22875895016044978,
            "max": 0.48489625530700897,
            "count": 25
        },
        "SAC_6_5.Policy.DiscreteEntropyCoeff.sum": {
            "value": 56.96097858995199,
            "min": 56.96097858995199,
            "max": 118.09378091390602,
            "count": 25
        },
        "SAC_6_5.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 25
        },
        "SAC_6_5.Policy.ContinuousEntropyCoeff.sum": {
            "value": 124.5,
            "min": 109.5,
            "max": 127.0,
            "count": 25
        },
        "SAC_6_5.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.00030000000000000003,
            "count": 25
        },
        "SAC_6_5.Policy.LearningRate.sum": {
            "value": 0.07469999999999999,
            "min": 0.0657,
            "max": 0.0762,
            "count": 25
        },
        "SAC_6_5.Environment.EpisodeLength.mean": {
            "value": 685.1428571428571,
            "min": 603.0,
            "max": 746.3846153846154,
            "count": 25
        },
        "SAC_6_5.Environment.EpisodeLength.sum": {
            "value": 9592.0,
            "min": 6498.0,
            "max": 12096.0,
            "count": 25
        },
        "SAC_6_5.Environment.CumulativeReward.mean": {
            "value": 775.1235925129482,
            "min": 649.1825127601624,
            "max": 795.7180255889892,
            "count": 25
        },
        "SAC_6_5.Environment.CumulativeReward.sum": {
            "value": 10851.730295181274,
            "min": 6588.520270347595,
            "max": 12463.440432548523,
            "count": 25
        },
        "SAC_6_5.Policy.ExtrinsicReward.mean": {
            "value": 775.1235925129482,
            "min": 649.1825127601624,
            "max": 795.7180255889892,
            "count": 25
        },
        "SAC_6_5.Policy.ExtrinsicReward.sum": {
            "value": 10851.730295181274,
            "min": 6588.520270347595,
            "max": 12463.440432548523,
            "count": 25
        },
        "SAC_6_5_3.Step.mean": {
            "value": 499943.0,
            "min": 259948.0,
            "max": 499943.0,
            "count": 25
        },
        "SAC_6_5_3.Step.sum": {
            "value": 499943.0,
            "min": 259948.0,
            "max": 499943.0,
            "count": 25
        },
        "SAC_6_5_3.Policy.ExtrinsicValue.mean": {
            "value": 24.23039436340332,
            "min": 17.161113739013672,
            "max": 24.4674129486084,
            "count": 25
        },
        "SAC_6_5_3.Policy.ExtrinsicValue.sum": {
            "value": 4046.475830078125,
            "min": 2728.6171875,
            "max": 4057.26806640625,
            "count": 25
        },
        "SAC_6_5_3.Losses.PolicyLoss.mean": {
            "value": -72.32293982039812,
            "min": -72.50224105326333,
            "max": -51.898576104416335,
            "count": 25
        },
        "SAC_6_5_3.Losses.PolicyLoss.sum": {
            "value": -18008.412015279133,
            "min": -18166.79555943807,
            "max": -11365.788166867178,
            "count": 25
        },
        "SAC_6_5_3.Losses.ValueLoss.mean": {
            "value": 0.04031698601981164,
            "min": 0.013197330813848477,
            "max": 0.04031698601981164,
            "count": 25
        },
        "SAC_6_5_3.Losses.ValueLoss.sum": {
            "value": 10.038929518933097,
            "min": 3.3653193575313614,
            "max": 10.079295057586084,
            "count": 25
        },
        "SAC_6_5_3.Losses.Q1Loss.mean": {
            "value": 0.7939115908929462,
            "min": 0.11396004686304442,
            "max": 0.7939115908929462,
            "count": 25
        },
        "SAC_6_5_3.Losses.Q1Loss.sum": {
            "value": 197.6839861323436,
            "min": 24.957250263006728,
            "max": 197.6839861323436,
            "count": 25
        },
        "SAC_6_5_3.Losses.Q2Loss.mean": {
            "value": 0.7920669837639114,
            "min": 0.11177452510136346,
            "max": 0.7920669837639114,
            "count": 25
        },
        "SAC_6_5_3.Losses.Q2Loss.sum": {
            "value": 197.22467895721394,
            "min": 24.478620997198597,
            "max": 197.22467895721394,
            "count": 25
        },
        "SAC_6_5_3.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.14474573620710987,
            "min": 0.14474573620710987,
            "max": 0.45661121826898987,
            "count": 25
        },
        "SAC_6_5_3.Policy.DiscreteEntropyCoeff.sum": {
            "value": 36.041688315570354,
            "min": 36.041688315570354,
            "max": 100.24026407023399,
            "count": 25
        },
        "SAC_6_5_3.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 25
        },
        "SAC_6_5_3.Policy.ContinuousEntropyCoeff.sum": {
            "value": 124.5,
            "min": 109.5,
            "max": 127.5,
            "count": 25
        },
        "SAC_6_5_3.Policy.LearningRate.mean": {
            "value": 0.0005000000000000001,
            "min": 0.0005000000000000001,
            "max": 0.0005000000000000002,
            "count": 25
        },
        "SAC_6_5_3.Policy.LearningRate.sum": {
            "value": 0.12450000000000003,
            "min": 0.10950000000000004,
            "max": 0.12750000000000003,
            "count": 25
        },
        "SAC_6_5_3.Environment.EpisodeLength.mean": {
            "value": 414.53846153846155,
            "min": 393.72727272727275,
            "max": 745.7692307692307,
            "count": 25
        },
        "SAC_6_5_3.Environment.EpisodeLength.sum": {
            "value": 10778.0,
            "min": 6563.0,
            "max": 12936.0,
            "count": 25
        },
        "SAC_6_5_3.Environment.CumulativeReward.mean": {
            "value": 487.86040350107044,
            "min": 451.4218411012129,
            "max": 825.0154033028163,
            "count": 25
        },
        "SAC_6_5_3.Environment.CumulativeReward.sum": {
            "value": 12684.370491027832,
            "min": 6700.520172119141,
            "max": 13495.120666503906,
            "count": 25
        },
        "SAC_6_5_3.Policy.ExtrinsicReward.mean": {
            "value": 487.86040350107044,
            "min": 451.4218411012129,
            "max": 825.0154033028163,
            "count": 25
        },
        "SAC_6_5_3.Policy.ExtrinsicReward.sum": {
            "value": 12684.370491027832,
            "min": 6700.520172119141,
            "max": 13495.120666503906,
            "count": 25
        },
        "SAC_6_5_2.Step.mean": {
            "value": 499951.0,
            "min": 259992.0,
            "max": 499951.0,
            "count": 25
        },
        "SAC_6_5_2.Step.sum": {
            "value": 499951.0,
            "min": 259992.0,
            "max": 499951.0,
            "count": 25
        },
        "SAC_6_5_2.Policy.ExtrinsicValue.mean": {
            "value": 23.258390426635742,
            "min": 17.017629623413086,
            "max": 23.858699798583984,
            "count": 25
        },
        "SAC_6_5_2.Policy.ExtrinsicValue.sum": {
            "value": 3837.634521484375,
            "min": 2790.89111328125,
            "max": 3936.685546875,
            "count": 25
        },
        "SAC_6_5_2.Environment.EpisodeLength.mean": {
            "value": 349.8695652173913,
            "min": 299.1111111111111,
            "max": 615.3157894736842,
            "count": 25
        },
        "SAC_6_5_2.Environment.EpisodeLength.sum": {
            "value": 8047.0,
            "min": 7514.0,
            "max": 12000.0,
            "count": 25
        },
        "SAC_6_5_2.Environment.CumulativeReward.mean": {
            "value": 384.4187117866848,
            "min": 314.17417897118463,
            "max": 599.8353150872623,
            "count": 25
        },
        "SAC_6_5_2.Environment.CumulativeReward.sum": {
            "value": 8841.63037109375,
            "min": 7018.660258293152,
            "max": 12411.750332921743,
            "count": 25
        },
        "SAC_6_5_2.Policy.ExtrinsicReward.mean": {
            "value": 384.4187117866848,
            "min": 314.17417897118463,
            "max": 599.8353150872623,
            "count": 25
        },
        "SAC_6_5_2.Policy.ExtrinsicReward.sum": {
            "value": 8841.63037109375,
            "min": 7018.660258293152,
            "max": 12411.750332921743,
            "count": 25
        },
        "SAC_6_5_2.Losses.PolicyLoss.mean": {
            "value": -65.73752174734909,
            "min": -70.6686570520895,
            "max": -53.92332378901998,
            "count": 25
        },
        "SAC_6_5_2.Losses.PolicyLoss.sum": {
            "value": -16368.642915089924,
            "min": -17737.832920074463,
            "max": -12186.671176318516,
            "count": 25
        },
        "SAC_6_5_2.Losses.ValueLoss.mean": {
            "value": 0.029712335710878793,
            "min": 0.015298650563694536,
            "max": 0.032077411073414615,
            "count": 25
        },
        "SAC_6_5_2.Losses.ValueLoss.sum": {
            "value": 7.39837159200882,
            "min": 3.824662640923634,
            "max": 8.051430179427069,
            "count": 25
        },
        "SAC_6_5_2.Losses.Q1Loss.mean": {
            "value": 0.5023953527002689,
            "min": 0.24914174403995276,
            "max": 0.5602363753914833,
            "count": 25
        },
        "SAC_6_5_2.Losses.Q1Loss.sum": {
            "value": 125.09644282236695,
            "min": 57.411553739287726,
            "max": 140.05909384787083,
            "count": 25
        },
        "SAC_6_5_2.Losses.Q2Loss.mean": {
            "value": 0.5029259771557457,
            "min": 0.24968763018399476,
            "max": 0.5648050001338124,
            "count": 25
        },
        "SAC_6_5_2.Losses.Q2Loss.sum": {
            "value": 125.22856831178069,
            "min": 57.109735274175925,
            "max": 141.2012500334531,
            "count": 25
        },
        "SAC_6_5_2.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.22072500937494888,
            "min": 0.22072500937494888,
            "max": 0.4807078749012589,
            "count": 25
        },
        "SAC_6_5_2.Policy.DiscreteEntropyCoeff.sum": {
            "value": 54.96052733436227,
            "min": 54.96052733436227,
            "max": 113.0066354200244,
            "count": 25
        },
        "SAC_6_5_2.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 25
        },
        "SAC_6_5_2.Policy.ContinuousEntropyCoeff.sum": {
            "value": 124.5,
            "min": 113.0,
            "max": 126.0,
            "count": 25
        },
        "SAC_6_5_2.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.00030000000000000003,
            "count": 25
        },
        "SAC_6_5_2.Policy.LearningRate.sum": {
            "value": 0.07469999999999999,
            "min": 0.0678,
            "max": 0.0756,
            "count": 25
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1643324450",
        "python_version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\alexc\\anaconda3\\envs\\mlagent_release_18\\Scripts\\mlagents-learn config/SAC/SAC_stepsUpdateBatch.yaml --run-id=SAC_stepsUpdateBatch_2 --env=builds/SAC_CarAgent6_multi --resume",
        "mlagents_version": "0.27.0",
        "mlagents_envs_version": "0.27.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.19.5",
        "end_time_seconds": "1643326488"
    },
    "total": 2037.9267484000002,
    "count": 1,
    "self": 0.3087639000002582,
    "children": {
        "run_training.setup": {
            "total": 0.07963289999999999,
            "count": 1,
            "self": 0.07963289999999999
        },
        "TrainerController.start_learning": {
            "total": 2037.5383516,
            "count": 1,
            "self": 0.699564100005091,
            "children": {
                "TrainerController._reset_env": {
                    "total": 5.7222166,
                    "count": 1,
                    "self": 5.7222166
                },
                "TrainerController.advance": {
                    "total": 2030.7785998999948,
                    "count": 28859,
                    "self": 0.8286705999851165,
                    "children": {
                        "env_step": {
                            "total": 517.0206534000131,
                            "count": 28859,
                            "self": 248.60305470001975,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 268.03296919999946,
                                    "count": 28859,
                                    "self": 4.292331100021784,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 263.7406380999777,
                                            "count": 83451,
                                            "self": 64.96823949997969,
                                            "children": {
                                                "TorchPolicy.sample_actions": {
                                                    "total": 198.77239859999798,
                                                    "count": 83451,
                                                    "self": 198.77239859999798
                                                }
                                            }
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.38462949999386,
                                    "count": 28859,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 2031.8934250000032,
                                            "count": 28859,
                                            "is_parallel": true,
                                            "self": 1843.4582355000023,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0013188,
                                                    "count": 3,
                                                    "is_parallel": true,
                                                    "self": 0.0005285999999999997,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0007902000000000002,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.0007902000000000002
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 188.43387070000088,
                                                    "count": 28859,
                                                    "is_parallel": true,
                                                    "self": 7.93296480001095,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 10.510749599991183,
                                                            "count": 28859,
                                                            "is_parallel": true,
                                                            "self": 10.510749599991183
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 138.3524595999995,
                                                            "count": 28859,
                                                            "is_parallel": true,
                                                            "self": 138.3524595999995
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 31.637696699999264,
                                                            "count": 86577,
                                                            "is_parallel": true,
                                                            "self": 11.351942899947684,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 20.28575380005158,
                                                                    "count": 346308,
                                                                    "is_parallel": true,
                                                                    "self": 20.28575380005158
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 1512.9292758999966,
                            "count": 86577,
                            "self": 3.367647999964902,
                            "children": {
                                "process_trajectory": {
                                    "total": 58.39181570001221,
                                    "count": 86577,
                                    "self": 56.500665900012116,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 1.891149800000096,
                                            "count": 18,
                                            "self": 1.891149800000096
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 1451.1698122000196,
                                    "count": 86121,
                                    "self": 0.6372578000530211,
                                    "children": {
                                        "SACTrainer._update_policy": {
                                            "total": 1450.5325543999666,
                                            "count": 86121,
                                            "self": 937.1292988999897,
                                            "children": {
                                                "TorchSACOptimizer.update": {
                                                    "total": 513.4032554999768,
                                                    "count": 18675,
                                                    "self": 513.4032554999768
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 7.000001005508238e-07,
                    "count": 1,
                    "self": 7.000001005508238e-07
                },
                "TrainerController._save_models": {
                    "total": 0.3379703000000518,
                    "count": 1,
                    "self": 0.02533630000039011,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.3126339999996617,
                            "count": 3,
                            "self": 0.3126339999996617
                        }
                    }
                }
            }
        }
    }
}