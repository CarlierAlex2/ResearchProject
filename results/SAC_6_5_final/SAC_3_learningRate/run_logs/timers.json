{
    "name": "root",
    "gauges": {
        "SAC_6_5_3.Policy.Entropy.mean": {
            "value": 0.9922598600387573,
            "min": 0.8227387070655823,
            "max": 4.402763843536377,
            "count": 50
        },
        "SAC_6_5_3.Policy.Entropy.sum": {
            "value": 10019.83984375,
            "min": 8189.541015625,
            "max": 44934.609375,
            "count": 50
        },
        "SAC_6_5_3.Step.mean": {
            "value": 499995.0,
            "min": 9950.0,
            "max": 499995.0,
            "count": 50
        },
        "SAC_6_5_3.Step.sum": {
            "value": 499995.0,
            "min": 9950.0,
            "max": 499995.0,
            "count": 50
        },
        "SAC_6_5_3.Policy.ExtrinsicValue.mean": {
            "value": 22.097618103027344,
            "min": 1.6648157835006714,
            "max": 22.097618103027344,
            "count": 50
        },
        "SAC_6_5_3.Policy.ExtrinsicValue.sum": {
            "value": 3646.10693359375,
            "min": 263.0408935546875,
            "max": 3646.10693359375,
            "count": 50
        },
        "SAC_6_5_3.Losses.PolicyLoss.mean": {
            "value": -63.801238628655945,
            "min": -64.66395752069381,
            "max": -5.586683259341949,
            "count": 50
        },
        "SAC_6_5_3.Losses.PolicyLoss.sum": {
            "value": -32028.221791585285,
            "min": -32372.647547730947,
            "max": -2407.86048477638,
            "count": 50
        },
        "SAC_6_5_3.Losses.ValueLoss.mean": {
            "value": 0.027861220955231525,
            "min": 0.002535803434111215,
            "max": 0.051978968063433356,
            "count": 50
        },
        "SAC_6_5_3.Losses.ValueLoss.sum": {
            "value": 13.986332919526225,
            "min": 1.2932597513967197,
            "max": 26.093441967843546,
            "count": 50
        },
        "SAC_6_5_3.Losses.Q1Loss.mean": {
            "value": 0.4825487021977702,
            "min": 0.10854407318226389,
            "max": 1.0699899526894567,
            "count": 50
        },
        "SAC_6_5_3.Losses.Q1Loss.sum": {
            "value": 242.23944850328064,
            "min": 55.35747732295459,
            "max": 531.78500648666,
            "count": 50
        },
        "SAC_6_5_3.Losses.Q2Loss.mean": {
            "value": 0.4950384401121383,
            "min": 0.10846033800759901,
            "max": 1.0875475869342277,
            "count": 50
        },
        "SAC_6_5_3.Losses.Q2Loss.sum": {
            "value": 248.50929693629342,
            "min": 55.31477238387549,
            "max": 540.5111507063111,
            "count": 50
        },
        "SAC_6_5_3.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.199870046351852,
            "min": 0.11113096404643284,
            "max": 0.45857585816189944,
            "count": 50
        },
        "SAC_6_5_3.Policy.DiscreteEntropyCoeff.sum": {
            "value": 100.3347632686297,
            "min": 55.343220095123556,
            "max": 197.64619486777866,
            "count": 50
        },
        "SAC_6_5_3.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 50
        },
        "SAC_6_5_3.Policy.ContinuousEntropyCoeff.sum": {
            "value": 251.0,
            "min": 215.5,
            "max": 255.0,
            "count": 50
        },
        "SAC_6_5_3.Policy.LearningRate.mean": {
            "value": 0.0005000000000000002,
            "min": 0.0005000000000000001,
            "max": 0.0005000000000000002,
            "count": 50
        },
        "SAC_6_5_3.Policy.LearningRate.sum": {
            "value": 0.2510000000000001,
            "min": 0.21550000000000008,
            "max": 0.2550000000000001,
            "count": 50
        },
        "SAC_6_5_3.Environment.EpisodeLength.mean": {
            "value": 548.6470588235294,
            "min": 173.75862068965517,
            "max": 749.0,
            "count": 50
        },
        "SAC_6_5_3.Environment.EpisodeLength.sum": {
            "value": 9327.0,
            "min": 6741.0,
            "max": 13436.0,
            "count": 50
        },
        "SAC_6_5_3.Environment.CumulativeReward.mean": {
            "value": 597.2253192733316,
            "min": 85.13221980465784,
            "max": 597.2253192733316,
            "count": 50
        },
        "SAC_6_5_3.Environment.CumulativeReward.sum": {
            "value": 10152.830427646637,
            "min": 766.1899782419205,
            "max": 12509.530376672745,
            "count": 50
        },
        "SAC_6_5_3.Policy.ExtrinsicReward.mean": {
            "value": 597.2253192733316,
            "min": 85.13221980465784,
            "max": 597.2253192733316,
            "count": 50
        },
        "SAC_6_5_3.Policy.ExtrinsicReward.sum": {
            "value": 10152.830427646637,
            "min": 766.1899782419205,
            "max": 12509.530376672745,
            "count": 50
        },
        "SAC_6_5_3.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "SAC_6_5_3.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "SAC_6_5.Policy.Entropy.mean": {
            "value": 1.2839972972869873,
            "min": 1.2839972972869873,
            "max": 4.600954055786133,
            "count": 50
        },
        "SAC_6_5.Policy.Entropy.sum": {
            "value": 12792.46484375,
            "min": 12792.46484375,
            "max": 46957.3359375,
            "count": 50
        },
        "SAC_6_5.Step.mean": {
            "value": 499975.0,
            "min": 9950.0,
            "max": 499975.0,
            "count": 50
        },
        "SAC_6_5.Step.sum": {
            "value": 499975.0,
            "min": 9950.0,
            "max": 499975.0,
            "count": 50
        },
        "SAC_6_5.Policy.ExtrinsicValue.mean": {
            "value": 22.068180084228516,
            "min": 0.9241281747817993,
            "max": 23.782251358032227,
            "count": 50
        },
        "SAC_6_5.Policy.ExtrinsicValue.sum": {
            "value": 3597.11328125,
            "min": 146.0122528076172,
            "max": 3876.507080078125,
            "count": 50
        },
        "SAC_6_5.Losses.PolicyLoss.mean": {
            "value": -66.48711872519608,
            "min": -68.34008584500268,
            "max": -3.8498565857268554,
            "count": 50
        },
        "SAC_6_5.Losses.PolicyLoss.sum": {
            "value": -33310.04648132324,
            "min": -34170.042922501336,
            "max": -1659.2881884482747,
            "count": 50
        },
        "SAC_6_5.Losses.ValueLoss.mean": {
            "value": 0.008962930783896483,
            "min": 0.0003784634456814636,
            "max": 0.09645047050534672,
            "count": 50
        },
        "SAC_6_5.Losses.ValueLoss.sum": {
            "value": 4.490428322732138,
            "min": 0.18998864973209473,
            "max": 41.57015278780444,
            "count": 50
        },
        "SAC_6_5.Losses.Q1Loss.mean": {
            "value": 0.5998196980154423,
            "min": 0.09422644768975005,
            "max": 0.8105271075777293,
            "count": 50
        },
        "SAC_6_5.Losses.Q1Loss.sum": {
            "value": 300.5096687057366,
            "min": 40.61159895428227,
            "max": 404.45302668128693,
            "count": 50
        },
        "SAC_6_5.Losses.Q2Loss.mean": {
            "value": 0.5952580797497781,
            "min": 0.09883476602293387,
            "max": 0.802629043137695,
            "count": 50
        },
        "SAC_6_5.Losses.Q2Loss.sum": {
            "value": 298.2242979546388,
            "min": 46.97263136593622,
            "max": 400.5118925257098,
            "count": 50
        },
        "SAC_6_5.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.11851386742409832,
            "min": 0.11851386742409832,
            "max": 0.49028947850078197,
            "count": 50
        },
        "SAC_6_5.Policy.DiscreteEntropyCoeff.sum": {
            "value": 59.37544757947326,
            "min": 59.37544757947326,
            "max": 240.95739642719755,
            "count": 50
        },
        "SAC_6_5.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 50
        },
        "SAC_6_5.Policy.ContinuousEntropyCoeff.sum": {
            "value": 250.5,
            "min": 215.5,
            "max": 255.0,
            "count": 50
        },
        "SAC_6_5.Policy.LearningRate.mean": {
            "value": 0.00010000000000000005,
            "min": 0.00010000000000000002,
            "max": 0.00010000000000000005,
            "count": 50
        },
        "SAC_6_5.Policy.LearningRate.sum": {
            "value": 0.05010000000000002,
            "min": 0.043100000000000006,
            "max": 0.05100000000000002,
            "count": 50
        },
        "SAC_6_5.Environment.EpisodeLength.mean": {
            "value": 541.8125,
            "min": 342.258064516129,
            "max": 749.0,
            "count": 50
        },
        "SAC_6_5.Environment.EpisodeLength.sum": {
            "value": 8669.0,
            "min": 6741.0,
            "max": 13482.0,
            "count": 50
        },
        "SAC_6_5.Environment.CumulativeReward.mean": {
            "value": 599.8641304689295,
            "min": 58.79888533221351,
            "max": 737.8926955789327,
            "count": 50
        },
        "SAC_6_5.Environment.CumulativeReward.sum": {
            "value": 10197.690217971802,
            "min": 529.1899679899216,
            "max": 12829.38038444519,
            "count": 50
        },
        "SAC_6_5.Policy.ExtrinsicReward.mean": {
            "value": 599.8641304689295,
            "min": 58.79888533221351,
            "max": 737.8926955789327,
            "count": 50
        },
        "SAC_6_5.Policy.ExtrinsicReward.sum": {
            "value": 10197.690217971802,
            "min": 529.1899679899216,
            "max": 12829.38038444519,
            "count": 50
        },
        "SAC_6_5.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "SAC_6_5.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "SAC_6_5_2.Policy.Entropy.mean": {
            "value": 0.9037557244300842,
            "min": 0.9037557244300842,
            "max": 4.324096202850342,
            "count": 50
        },
        "SAC_6_5_2.Policy.Entropy.sum": {
            "value": 8971.5830078125,
            "min": 8971.5830078125,
            "max": 44131.7265625,
            "count": 50
        },
        "SAC_6_5_2.Step.mean": {
            "value": 499965.0,
            "min": 9950.0,
            "max": 499965.0,
            "count": 50
        },
        "SAC_6_5_2.Step.sum": {
            "value": 499965.0,
            "min": 9950.0,
            "max": 499965.0,
            "count": 50
        },
        "SAC_6_5_2.Policy.ExtrinsicValue.mean": {
            "value": 20.165618896484375,
            "min": 0.9262316823005676,
            "max": 21.54322624206543,
            "count": 50
        },
        "SAC_6_5_2.Policy.ExtrinsicValue.sum": {
            "value": 3327.3271484375,
            "min": 146.3446044921875,
            "max": 3640.80517578125,
            "count": 50
        },
        "SAC_6_5_2.Losses.PolicyLoss.mean": {
            "value": -61.34976717727298,
            "min": -63.73667361497126,
            "max": -4.0817431729418345,
            "count": 50
        },
        "SAC_6_5_2.Losses.PolicyLoss.sum": {
            "value": -30674.88358863649,
            "min": -31761.124670128593,
            "max": -1759.2313075379307,
            "count": 50
        },
        "SAC_6_5_2.Losses.ValueLoss.mean": {
            "value": 0.023418190508229395,
            "min": 0.0023816876893964466,
            "max": 0.11634751860150996,
            "count": 50
        },
        "SAC_6_5_2.Losses.ValueLoss.sum": {
            "value": 11.709095254114697,
            "min": 1.231332535417963,
            "max": 50.14578051725079,
            "count": 50
        },
        "SAC_6_5_2.Losses.Q1Loss.mean": {
            "value": 0.5840728150180762,
            "min": 0.1250038255840921,
            "max": 0.8907658242585329,
            "count": 50
        },
        "SAC_6_5_2.Losses.Q1Loss.sum": {
            "value": 292.0364075090381,
            "min": 64.62697782697562,
            "max": 448.9459754263006,
            "count": 50
        },
        "SAC_6_5_2.Losses.Q2Loss.mean": {
            "value": 0.5836911268000802,
            "min": 0.12223615844650082,
            "max": 0.8893720326674166,
            "count": 50
        },
        "SAC_6_5_2.Losses.Q2Loss.sum": {
            "value": 291.8455634000401,
            "min": 63.19609391684092,
            "max": 448.24350446437796,
            "count": 50
        },
        "SAC_6_5_2.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.18078624330539078,
            "min": 0.12907442568086444,
            "max": 0.4746823848727397,
            "count": 50
        },
        "SAC_6_5_2.Policy.DiscreteEntropyCoeff.sum": {
            "value": 90.39312165269538,
            "min": 64.53721284043222,
            "max": 221.73012674551347,
            "count": 50
        },
        "SAC_6_5_2.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 50
        },
        "SAC_6_5_2.Policy.ContinuousEntropyCoeff.sum": {
            "value": 250.0,
            "min": 215.5,
            "max": 258.5,
            "count": 50
        },
        "SAC_6_5_2.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.00030000000000000003,
            "count": 50
        },
        "SAC_6_5_2.Policy.LearningRate.sum": {
            "value": 0.15,
            "min": 0.1293,
            "max": 0.15510000000000002,
            "count": 50
        },
        "SAC_6_5_2.Environment.EpisodeLength.mean": {
            "value": 428.75,
            "min": 199.2,
            "max": 749.0,
            "count": 50
        },
        "SAC_6_5_2.Environment.EpisodeLength.sum": {
            "value": 8575.0,
            "min": 6741.0,
            "max": 12372.0,
            "count": 50
        },
        "SAC_6_5_2.Environment.CumulativeReward.mean": {
            "value": 454.68751058578493,
            "min": 115.97110898958312,
            "max": 498.45592790300196,
            "count": 50
        },
        "SAC_6_5_2.Environment.CumulativeReward.sum": {
            "value": 9093.750211715698,
            "min": 1043.739980906248,
            "max": 11900.48032951355,
            "count": 50
        },
        "SAC_6_5_2.Policy.ExtrinsicReward.mean": {
            "value": 454.68751058578493,
            "min": 115.97110898958312,
            "max": 498.45592790300196,
            "count": 50
        },
        "SAC_6_5_2.Policy.ExtrinsicReward.sum": {
            "value": 9093.750211715698,
            "min": 1043.739980906248,
            "max": 11900.48032951355,
            "count": 50
        },
        "SAC_6_5_2.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "SAC_6_5_2.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1643449942",
        "python_version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\alexc\\anaconda3\\envs\\mlagent_release_18\\Scripts\\mlagents-learn config/SAC/SAC_03_learningRate.yaml --run-id=SAC_03_learningRate --env=builds/SAC_CarAgent6_multi",
        "mlagents_version": "0.27.0",
        "mlagents_envs_version": "0.27.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.19.5",
        "end_time_seconds": "1643456887"
    },
    "total": 6945.6137913,
    "count": 1,
    "self": 2.7148692000000665,
    "children": {
        "run_training.setup": {
            "total": 0.2445364,
            "count": 1,
            "self": 0.2445364
        },
        "TrainerController.start_learning": {
            "total": 6942.654385700001,
            "count": 1,
            "self": 1.4683468000066568,
            "children": {
                "TrainerController._reset_env": {
                    "total": 18.7661619,
                    "count": 1,
                    "self": 18.7661619
                },
                "TrainerController.advance": {
                    "total": 6922.027839599994,
                    "count": 58113,
                    "self": 1.7595536999560863,
                    "children": {
                        "env_step": {
                            "total": 1253.6228661999733,
                            "count": 58113,
                            "self": 561.2585747999558,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 691.576851299946,
                                    "count": 58113,
                                    "self": 8.725309899860918,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 682.8515414000851,
                                            "count": 166779,
                                            "self": 195.13675340006034,
                                            "children": {
                                                "TorchPolicy.sample_actions": {
                                                    "total": 487.7147880000248,
                                                    "count": 166779,
                                                    "self": 487.7147880000248
                                                }
                                            }
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.7874401000714073,
                                    "count": 58113,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 6931.527129800073,
                                            "count": 58113,
                                            "is_parallel": true,
                                            "self": 6513.6472171001005,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0013271000000000001,
                                                    "count": 3,
                                                    "is_parallel": true,
                                                    "self": 0.00047980000000000006,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0008473,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.0008473
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 417.87858559997323,
                                                    "count": 58113,
                                                    "is_parallel": true,
                                                    "self": 16.343871299954344,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 21.58849729993729,
                                                            "count": 58113,
                                                            "is_parallel": true,
                                                            "self": 21.58849729993729
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 316.4088264999383,
                                                            "count": 58113,
                                                            "is_parallel": true,
                                                            "self": 316.4088264999383
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 63.53739050014334,
                                                            "count": 174339,
                                                            "is_parallel": true,
                                                            "self": 22.595501300158283,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 40.941889199985056,
                                                                    "count": 697356,
                                                                    "is_parallel": true,
                                                                    "self": 40.941889199985056
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 5666.645419700065,
                            "count": 174339,
                            "self": 7.081821600225339,
                            "children": {
                                "process_trajectory": {
                                    "total": 133.41963880000176,
                                    "count": 174339,
                                    "self": 130.03300920000075,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 3.386629600001015,
                                            "count": 30,
                                            "self": 3.386629600001015
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 5526.143959299838,
                                    "count": 173941,
                                    "self": 1.3643936997477795,
                                    "children": {
                                        "SACTrainer._update_policy": {
                                            "total": 5524.77956560009,
                                            "count": 173941,
                                            "self": 3232.560873600189,
                                            "children": {
                                                "TorchSACOptimizer.update": {
                                                    "total": 2292.218691999901,
                                                    "count": 74845,
                                                    "self": 2292.218691999901
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 7.999997251317836e-07,
                    "count": 1,
                    "self": 7.999997251317836e-07
                },
                "TrainerController._save_models": {
                    "total": 0.39203660000021046,
                    "count": 1,
                    "self": 0.047251399999368005,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.34478520000084245,
                            "count": 3,
                            "self": 0.34478520000084245
                        }
                    }
                }
            }
        }
    }
}